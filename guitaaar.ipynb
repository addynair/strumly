{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee533be9-8590-4e55-92ef-beb724b56280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762541752.259225   52036 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1762541752.266469   52199 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.1), renderer: AMD Radeon Vega 3 Graphics (radeonsi, raven2, ACO, DRM 3.61, 6.14.0-33-generic)\n",
      "W0000 00:00:1762541752.332011   52194 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762541752.405535   52194 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "[ WARN:0@10.421] global cap_v4l.cpp:914 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@10.422] global obsensor_uvc_stream_channel.cpp:163 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pygame\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Init\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "pygame.init()\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load music files\n",
    "chords = {\n",
    "    0: pygame.mixer.Sound(\"sounds/1.mp3\"),\n",
    "    1: pygame.mixer.Sound(\"sounds/2.mp3\"),\n",
    "    2: pygame.mixer.Sound(\"sounds/EM.mp3\"),\n",
    "    3: pygame.mixer.Sound(\"sounds/GM.mp3\"),\n",
    "    4: pygame.mixer.Sound(\"sounds/3.mp3\"),\n",
    "    5: pygame.mixer.Sound(\"sounds/4.mp3\")\n",
    "}\n",
    "\n",
    "# Count fingers\n",
    "def count_fingers(hand_landmarks, hand_type):\n",
    "    tips = [4, 8, 12, 16, 20]\n",
    "    fingers = []\n",
    "\n",
    "    # Thumb\n",
    "    if hand_type == \"Right\":\n",
    "        fingers.append(hand_landmarks.landmark[tips[0]].x < hand_landmarks.landmark[tips[0] - 1].x)\n",
    "    else:\n",
    "        fingers.append(hand_landmarks.landmark[tips[0]].x > hand_landmarks.landmark[tips[0] - 1].x)\n",
    "\n",
    "    # 4 fingers\n",
    "    for tip in tips[1:]:\n",
    "        fingers.append(hand_landmarks.landmark[tip].y < hand_landmarks.landmark[tip - 2].y)\n",
    "\n",
    "    return fingers.count(True)\n",
    "\n",
    "# Detect strum = right hand vel\n",
    "last_strum_y = None\n",
    "last_strum_time = 0\n",
    "def detect_strum(y):\n",
    "    global last_strum_y, last_strum_time\n",
    "    if last_strum_y is None:\n",
    "        last_strum_y = y\n",
    "        return False\n",
    "\n",
    "    speed = abs(y - last_strum_y)\n",
    "    last_strum_y = y\n",
    "\n",
    "    current_time = time.time()\n",
    "    if speed > 0.03 and current_time - last_strum_time > 0.5:\n",
    "        last_strum_time = current_time\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "current_chord = 0\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks and results.multi_handedness:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            hand_type = handedness.classification[0].label\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            cx = int(hand_landmarks.landmark[0].x * img.shape[1])\n",
    "            cy = int(hand_landmarks.landmark[0].y * img.shape[0])\n",
    "\n",
    "            if hand_type == \"Left\":\n",
    "                # Left hand = chord\n",
    "                current_chord = count_fingers(hand_landmarks, hand_type)\n",
    "                cv2.putText(img, f\"Chord: {current_chord}\", (10, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            else:\n",
    "                # Right hand = strum\n",
    "                if detect_strum(hand_landmarks.landmark[0].y):\n",
    "                    chords[current_chord].play()\n",
    "                    cv2.putText(img, \"Strum!\", (400, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Air Guitar ðŸŽ¸\", img)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9678ad-1c79-437a-92b7-132b5d622884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:guitar]",
   "language": "python",
   "name": "conda-env-guitar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
